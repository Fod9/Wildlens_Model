{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T11:56:10.812199Z",
     "start_time": "2025-06-28T11:56:08.860347Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "project_root = Path(\"../../\").resolve()\n",
    "sys.path.append(str(project_root))\n",
    "from scripts.yolo_finetuning.yolo_inference import YOLOInference\n",
    "\n",
    "\n",
    "device = \"cuda\" if tf.config.list_physical_devices(\"GPU\") else \"cpu\"\n",
    "\n",
    "# Set memory growth for GPU\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "print(f\"GPUs available: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "\n",
    "# Initialize YOLO preprocessor as specified in MobileNet_training.md\n",
    "yolo_model_path = \"../../notebooks/yolo/best_so_far.pt\"\n",
    "if Path(yolo_model_path).exists():\n",
    "    try:\n",
    "        yolo_inference = YOLOInference(yolo_model_path, conf_threshold=0.25, iou_threshold=0.45)\n",
    "        print(f\"✓ YOLO model loaded: {yolo_model_path}\")\n",
    "        use_yolo_preprocessing = True\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ YOLO loading failed: {e}\")\n",
    "        use_yolo_preprocessing = False\n",
    "else:\n",
    "    print(f\"⚠ YOLO model not found at: {yolo_model_path}\")\n",
    "    use_yolo_preprocessing = False"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-28 13:56:08.971519: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-28 13:56:08.979508: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-28 13:56:08.988755: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-28 13:56:08.991593: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-28 13:56:08.998778: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-28 13:56:09.480595: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/spokay/miniconda3/envs/yolo_end/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ YOLO inference imported successfully\n",
      "GPUs available: 0\n",
      "✓ YOLO model loaded: ../../notebooks/yolo/best_so_far.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-28 13:56:10.793500: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2025-06-28 13:56:10.793529: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:135] retrieving CUDA diagnostic information for host: 2a01cb0001104f00845b1dbbc3600f10.ipv6.abo.wanadoo.fr\n",
      "2025-06-28 13:56:10.793543: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:142] hostname: 2a01cb0001104f00845b1dbbc3600f10.ipv6.abo.wanadoo.fr\n",
      "2025-06-28 13:56:10.793726: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:166] libcuda reported version is: 570.153.2\n",
      "2025-06-28 13:56:10.793745: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] kernel reported version is: NOT_FOUND: could not find kernel module information in driver version file contents: \"NVRM version: NVIDIA UNIX Open Kernel Module for x86_64  570.153.02  Release Build  (dvs-builder@U22-A23-20-3)  Tue May 13 16:34:58 UTC 2025\n",
      "GCC version:  gcc version 15.1.1 20250521 (Red Hat 15.1.1-2) (GCC) \n",
      "\"\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "# YOLO Preprocessing Functions (as specified in MobileNet_training.md)\n\ndef preprocess_image_with_yolo(image_path, target_size=(224, 224)):\n    \"\"\"\n    Apply YOLO preprocessing to get footprint crops, with center crop fallback.\n    This implements the requirement from MobileNet_training.md.\n    \"\"\"\n    if not use_yolo_preprocessing:\n        # Fallback: center crop\n        from PIL import Image\n        img = Image.open(image_path)\n        width, height = img.size\n        size = min(width, height)\n        left, top = (width - size) // 2, (height - size) // 2\n        cropped = img.crop((left, top, left + size, top + size))\n        resized = cropped.resize(target_size)\n        return np.array(resized)\n    \n    try:\n        # Use YOLO detection as specified in requirements\n        result = yolo_inference.infer_and_get_best_crop(str(image_path))\n        \n        if result is not None:\n            bbox, cropped_image = result\n            # Resize to target size for MobileNet\n            from PIL import Image\n            cropped_pil = Image.fromarray(cropped_image)\n            resized_image = cropped_pil.resize(target_size)\n            return np.array(resized_image)\n        else:\n            # Fallback to center crop if YOLO fails\n            from PIL import Image\n            img = Image.open(image_path)\n            width, height = img.size\n            size = min(width, height)\n            left, top = (width - size) // 2, (height - size) // 2\n            cropped = img.crop((left, top, left + size, top + size))\n            resized = cropped.resize(target_size)\n            return np.array(resized)\n            \n    except Exception as e:\n        print(f\"YOLO processing failed for {image_path}: {e}\")\n        # Fallback to center crop\n        from PIL import Image\n        img = Image.open(image_path)\n        width, height = img.size\n        size = min(width, height)\n        left, top = (width - size) // 2, (height - size) // 2\n        cropped = img.crop((left, top, left + size, top + size))\n        resized = cropped.resize(target_size)\n        return np.array(resized)\n\ndef create_yolo_preprocessed_dataset(data_dir, batch_size=16, validation_split=None):\n    \"\"\"\n    Create dataset with YOLO preprocessing as specified in MobileNet_training.md.\n    \"\"\"\n    from pathlib import Path\n    \n    data_path = Path(data_dir)\n    image_paths = []\n    labels = []\n    class_names = []\n    \n    # Collect images and labels\n    for class_dir in sorted(data_path.iterdir()):\n        if class_dir.is_dir():\n            class_names.append(class_dir.name)\n            class_idx = len(class_names) - 1\n            \n            for img_path in class_dir.glob(\"*.jpg\"):\n                image_paths.append(str(img_path))\n                labels.append(class_idx)\n    \n    print(f\"Found {len(image_paths)} images across {len(class_names)} classes\")\n    print(f\"Classes: {class_names}\")\n    \n    def preprocess_function(image_path, label):\n        \"\"\"Apply YOLO preprocessing.\"\"\"\n        processed_image = preprocess_image_with_yolo(image_path.numpy().decode('utf-8'))\n        return processed_image.astype(np.float32), label\n    \n    # Create TensorFlow dataset\n    path_ds = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n    \n    # Apply YOLO preprocessing\n    dataset = path_ds.map(\n        lambda path, label: tf.py_function(\n            preprocess_function, \n            [path, label], \n            [tf.float32, tf.int32]\n        ),\n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n    \n    # Set shapes and normalize\n    dataset = dataset.map(lambda img, label: (tf.reshape(img, [224, 224, 3]), label))\n    dataset = dataset.map(lambda img, label: (img / 255.0, label))\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    \n    return dataset, class_names\n\nprint(\"✓ YOLO preprocessing functions ready\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T09:53:35.614473Z",
     "start_time": "2025-06-28T09:53:35.609660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ YOLO preprocessing functions ready\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T09:53:36.685588Z",
     "start_time": "2025-06-28T09:53:36.616542Z"
    }
   },
   "source": "# Load OAT datasets with YOLO preprocessing (as specified in MobileNet_training.md)\nprint(\"Loading OAT datasets with YOLO preprocessing...\")\n\nif use_yolo_preprocessing:\n    print(\"Using YOLO preprocessing for all datasets\")\n    \n    # Use YOLO preprocessing\n    train_ds, oat_class_names = create_yolo_preprocessed_dataset(\n        \"../../data/OpenAnimalTracks_spokay/cropped_imgs/train\",\n        batch_size=16\n    )\n    \n    val_ds, _ = create_yolo_preprocessed_dataset(\n        \"../../data/OpenAnimalTracks_spokay/cropped_imgs/val\",\n        batch_size=16\n    )\n    \n    test_ds, _ = create_yolo_preprocessed_dataset(\n        \"../../data/OpenAnimalTracks_spokay/cropped_imgs/test\",\n        batch_size=16\n    )\n    \nelse:\n    print(\"Fallback: Using standard image loading\")\n    \n    # Fallback to standard loading\n    train_ds = keras.preprocessing.image_dataset_from_directory(\n        \"../../data/OpenAnimalTracks_spokay/cropped_imgs/train\",\n        image_size=(224, 224),\n        batch_size=16,\n    )\n    test_ds = keras.preprocessing.image_dataset_from_directory(\n        \"../../data/OpenAnimalTracks_spokay/cropped_imgs/test\",\n        image_size=(224, 224),\n        batch_size=16,\n    )\n    val_ds = keras.preprocessing.image_dataset_from_directory(\n        \"../../data/OpenAnimalTracks_spokay/cropped_imgs/val\",\n        image_size=(224, 224),\n        batch_size=16,\n    )\n    oat_class_names = train_ds.class_names\n\nprint(f\"OAT dataset loaded: {len(oat_class_names)} classes\")\nprint(f\"Classes: {oat_class_names}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OAT datasets with YOLO preprocessing...\n",
      "Using YOLO preprocessing for all datasets\n",
      "Found 2514 images across 18 classes\n",
      "Classes: ['beaver', 'black_bear', 'bob_cat', 'coyote', 'elephant', 'goose', 'gray_fox', 'horse', 'lion', 'mink', 'mouse', 'muledeer', 'otter', 'raccoon', 'rat', 'skunk', 'turkey', 'western_grey_squirrel']\n",
      "Found 346 images across 18 classes\n",
      "Classes: ['beaver', 'black_bear', 'bob_cat', 'coyote', 'elephant', 'goose', 'gray_fox', 'horse', 'lion', 'mink', 'mouse', 'muledeer', 'otter', 'raccoon', 'rat', 'skunk', 'turkey', 'western_grey_squirrel']\n",
      "Found 719 images across 18 classes\n",
      "Classes: ['beaver', 'black_bear', 'bob_cat', 'coyote', 'elephant', 'goose', 'gray_fox', 'horse', 'lion', 'mink', 'mouse', 'muledeer', 'otter', 'raccoon', 'rat', 'skunk', 'turkey', 'western_grey_squirrel']\n",
      "OAT dataset loaded: 18 classes\n",
      "Classes: ['beaver', 'black_bear', 'bob_cat', 'coyote', 'elephant', 'goose', 'gray_fox', 'horse', 'lion', 'mink', 'mouse', 'muledeer', 'otter', 'raccoon', 'rat', 'skunk', 'turkey', 'western_grey_squirrel']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T09:53:40.001523Z",
     "start_time": "2025-06-28T09:53:39.674679Z"
    }
   },
   "source": [
    "mobilenet = keras.applications.MobileNetV3Small(\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    ")\n",
    "\n",
    "# Initially freeze the base model\n",
    "mobilenet.trainable = False\n",
    "\n",
    "# Data augmentation\n",
    "data_augmentation = keras.Sequential([\n",
    "    keras.layers.RandomFlip(\"horizontal\"),\n",
    "    keras.layers.RandomRotation(0.1),\n",
    "    keras.layers.RandomZoom(0.1),\n",
    "    keras.layers.RandomContrast(0.1),\n",
    "])\n",
    "\n",
    "# Build the complete model\n",
    "model = keras.Sequential([\n",
    "    data_augmentation,\n",
    "    mobilenet,\n",
    "    keras.layers.GlobalAveragePooling2D(),\n",
    "    keras.layers.Dense(512, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(256, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(18, activation=\"softmax\"),\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T09:53:41.836796Z",
     "start_time": "2025-06-28T09:53:41.017357Z"
    }
   },
   "source": [
    "\n",
    "model.compile(optimizer=\"adamax\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(train_ds, epochs=50, validation_data=val_ds, callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3)])\n",
    "\n",
    "# Save the model\n",
    "model.save(\"mobilenet.keras\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot take the length of shape with unknown rank.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m model.compile(optimizer=\u001B[33m\"\u001B[39m\u001B[33madamax\u001B[39m\u001B[33m\"\u001B[39m, loss=\u001B[33m\"\u001B[39m\u001B[33msparse_categorical_crossentropy\u001B[39m\u001B[33m\"\u001B[39m, metrics=[\u001B[33m\"\u001B[39m\u001B[33maccuracy\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m model.fit(train_ds, epochs=\u001B[32m50\u001B[39m, validation_data=val_ds, callbacks=[keras.callbacks.EarlyStopping(monitor=\u001B[33m\"\u001B[39m\u001B[33mval_loss\u001B[39m\u001B[33m\"\u001B[39m, patience=\u001B[32m3\u001B[39m)])\n\u001B[32m      5\u001B[39m \u001B[38;5;66;03m# Save the model\u001B[39;00m\n\u001B[32m      6\u001B[39m model.save(\u001B[33m\"\u001B[39m\u001B[33mmobilenet.keras\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/yolo_end/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001B[39m, in \u001B[36mfilter_traceback.<locals>.error_handler\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    119\u001B[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001B[32m    120\u001B[39m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[32m    121\u001B[39m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m122\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e.with_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    123\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    124\u001B[39m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/yolo_end/lib/python3.12/site-packages/optree/ops.py:766\u001B[39m, in \u001B[36mtree_map\u001B[39m\u001B[34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001B[39m\n\u001B[32m    764\u001B[39m leaves, treespec = _C.flatten(tree, is_leaf, none_is_leaf, namespace)\n\u001B[32m    765\u001B[39m flat_args = [leaves] + [treespec.flatten_up_to(r) \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m rests]\n\u001B[32m--> \u001B[39m\u001B[32m766\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m treespec.unflatten(\u001B[38;5;28mmap\u001B[39m(func, *flat_args))\n",
      "\u001B[31mValueError\u001B[39m: Cannot take the length of shape with unknown rank."
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "mobilenet_loaded = keras.models.load_model(\"mobilenet.keras\")\nmobilenet.trainable = True\n\nfine_tune_at = len(mobilenet.layers) - 20\n\nfor layer in mobilenet.layers[:fine_tune_at]:\n    layer.trainable = False\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-5), \n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"]\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m158/158\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 11ms/step - accuracy: 0.4119 - loss: 2.9213 - val_accuracy: 0.4306 - val_loss: 2.8900\n",
      "Epoch 2/10\n",
      "\u001B[1m158/158\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.4416 - loss: 2.7654 - val_accuracy: 0.4306 - val_loss: 2.9450\n",
      "Epoch 3/10\n",
      "\u001B[1m158/158\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.4766 - loss: 2.6233 - val_accuracy: 0.4335 - val_loss: 2.9369\n",
      "Epoch 4/10\n",
      "\u001B[1m158/158\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.4886 - loss: 2.5909 - val_accuracy: 0.4393 - val_loss: 2.9169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x712a3a55ac30>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=10, validation_data=val_ds, callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Load Real dataset with YOLO preprocessing (as specified in MobileNet_training.md)\nprint(\"Loading Real dataset with YOLO preprocessing...\")\n\nif use_yolo_preprocessing:\n    print(\"Using YOLO preprocessing for Real dataset\")\n    \n    # Load full dataset first to get all images\n    real_ds_full, real_class_names = create_yolo_preprocessed_dataset(\n        \"../../data/dataset_no_oat_downsample_spokay\",\n        batch_size=32\n    )\n    \n    # Split into train and validation\n    train_size = int(0.8 * len(list(real_ds_full)))\n    val_size = len(list(real_ds_full)) - train_size\n    \n    real_train_ds = real_ds_full.take(train_size)\n    real_val_ds = real_ds_full.skip(train_size)\n    \nelse:\n    print(\"Fallback: Using standard image loading for Real dataset\")\n    \n    real_ds = keras.preprocessing.image_dataset_from_directory(\n        \"../../data/dataset_no_oat_downsample_spokay\",\n        image_size=(224, 224),\n        batch_size=32,\n    )\n    \n    real_class_names = real_ds.class_names\n    \n    # Split into train and validation\n    train_size = int(0.8 * len(real_ds))\n    val_size = len(real_ds) - train_size\n    \n    real_train_ds = real_ds.take(train_size)\n    real_val_ds = real_ds.skip(train_size)\n\nprint(f\"Real dataset loaded: {len(real_class_names)} classes\")\nprint(f\"Classes: {real_class_names}\")\nprint(f\"Dataset split - Training batches: {train_size}, Validation batches: {val_size}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset d'entraînement: 28 batches\n",
      "Dataset de validation: 7 batches\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_size = int(0.8 * len(real_ds))\n",
    "val_size = len(real_ds) - train_size\n",
    "\n",
    "real_train_ds = real_ds.take(train_size)\n",
    "real_val_ds = real_ds.skip(train_size)\n",
    "\n",
    "print(f\"Dataset d'entraînement: {train_size} batches\")\n",
    "print(f\"Dataset de validation: {val_size} batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "print(\"Début du fine-tuning...\")\n\nhistory = model.fit(\n    real_train_ds,\n    epochs=60,\n    validation_data=real_val_ds,\n    callbacks=[\n        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True),\n        keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-7)\n    ]\n)\n\nmodel.save(\"mobilenet_finetuned.keras\")\nprint(\"Modèle fine-tuné sauvegardé sous 'mobilenet_finetuned.keras'\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Évaluation du modèle fine-tuné\nimport matplotlib.pyplot as plt\n\n# Afficher les courbes d'apprentissage\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Perte du modèle')\nplt.xlabel('Époque')\nplt.ylabel('Perte')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Précision du modèle')\nplt.xlabel('Époque')\nplt.ylabel('Précision')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Évaluer sur l'ensemble de validation\nval_loss, val_accuracy = model.evaluate(real_val_ds)\nprint(f\"Précision finale sur les données de validation: {val_accuracy:.4f}\")\nprint(f\"Perte finale sur les données de validation: {val_loss:.4f}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

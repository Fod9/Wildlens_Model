{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# MobileNet Two-Stage Training Pipeline\n",
    "\n",
    "Implementation of the two-stage MobileNet training pipeline with mandatory YOLO preprocessing as specified in MobileNet_training.md.\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **Stage 1**: Train MobileNet on OAT dataset (18 classes) with YOLO preprocessing\n",
    "2. **Stage 2**: Fine-tune on Real dataset (13 classes) with architecture adaptation\n",
    "\n",
    "## Key Features:\n",
    "- Mandatory YOLO preprocessing for footprint detection and cropping\n",
    "- Proper architecture adaptation between stages (18â†’13 classes)\n",
    "- Backbone weight transfer with classification head rebuilding\n",
    "- Comprehensive evaluation and visualization"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T10:43:02.875832Z",
     "start_time": "2025-06-29T10:43:01.970546Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = Path().resolve().parent.parent\n",
    "sys.path.append(str(project_root))\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 12:43:02.068578: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-29 12:43:02.075935: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751193782.084864   10810 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751193782.087474   10810 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-29 12:43:02.097056: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/spokay/miniconda3/envs/yolo_end/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T10:43:04.576163Z",
     "start_time": "2025-06-29T10:43:04.457859Z"
    }
   },
   "source": [
    "# Configure GPU\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU detected: {len(gpus)} device(s)\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU configuration error: {e}\")\n",
    "else:\n",
    "    print(\"No GPU detected, using CPU\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU detected: 1 device(s)\n",
      "TensorFlow version: 2.18.0\n",
      "Keras version: 3.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1751193784.574607   10810 gpu_device.cc:2433] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO Preprocessing Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T10:43:07.605823Z",
     "start_time": "2025-06-29T10:43:07.561955Z"
    }
   },
   "source": [
    "# Initialize YOLO inference for preprocessing\n",
    "from scripts.yolo_finetuning.yolo_inference import YOLOInference\n",
    "\n",
    "yolo_model_path = \"../yolo/best_so_far.onnx\"\n",
    "yolo_inference = YOLOInference(\n",
    "    model_path=yolo_model_path,\n",
    ")\n",
    "print(\"YOLO inference initialized for preprocessing\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO inference initialized for preprocessing\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T10:43:08.807412Z",
     "start_time": "2025-06-29T10:43:08.802570Z"
    }
   },
   "source": [
    "def preprocess_image_with_yolo(image_path, target_size=(224, 224)):\n",
    "    try:\n",
    "        # Convert tensor to string if needed\n",
    "        if isinstance(image_path, tf.Tensor):\n",
    "            image_path = image_path.numpy().decode('utf-8')\n",
    "        elif isinstance(image_path, bytes):\n",
    "            image_path = image_path.decode('utf-8')\n",
    "        \n",
    "        # Get YOLO crop - this returns a tuple (bbox, cropped_image)\n",
    "        result = yolo_inference.infer_and_get_best_crop(str(image_path))\n",
    "        \n",
    "        if result is not None:\n",
    "            bbox, cropped_image = result\n",
    "            if cropped_image is not None and cropped_image.shape[0] > 0 and cropped_image.shape[1] > 0:\n",
    "                # Use YOLO crop\n",
    "                image = Image.fromarray(cropped_image)\n",
    "            else:\n",
    "                # Fallback to center crop\n",
    "                image = Image.open(image_path)\n",
    "                # Center crop to square\n",
    "                min_dim = min(image.size)\n",
    "                left = (image.size[0] - min_dim) // 2\n",
    "                top = (image.size[1] - min_dim) // 2\n",
    "                image = image.crop((left, top, left + min_dim, top + min_dim))\n",
    "        else:\n",
    "            # Fallback to center crop\n",
    "            image = Image.open(image_path)\n",
    "            # Center crop to square\n",
    "            min_dim = min(image.size)\n",
    "            left = (image.size[0] - min_dim) // 2\n",
    "            top = (image.size[1] - min_dim) // 2\n",
    "            image = image.crop((left, top, left + min_dim, top + min_dim))\n",
    "        \n",
    "        # Resize to target size\n",
    "        image = image.resize(target_size)\n",
    "        \n",
    "        # Convert to RGB if needed\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        # Convert to numpy array and normalize\n",
    "        image_array = np.array(image, dtype=np.float32)\n",
    "        image_array = image_array / 255.0\n",
    "        \n",
    "        return image_array\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        # Return a black image as fallback\n",
    "        return np.zeros((*target_size, 3), dtype=np.float32)\n",
    "\n",
    "def create_yolo_preprocessed_dataset(data_dir, class_names, batch_size=16, shuffle=True):\n",
    "    def load_and_preprocess_image(path, label):\n",
    "        image = tf.py_function(\n",
    "            func=preprocess_image_with_yolo,\n",
    "            inp=[path],\n",
    "            Tout=tf.float32\n",
    "        )\n",
    "        image.set_shape([224, 224, 3])\n",
    "        return image, label\n",
    "    \n",
    "    # Get all image paths and labels\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        if os.path.exists(class_dir):\n",
    "            for img_file in os.listdir(class_dir):\n",
    "                if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    image_paths.append(os.path.join(class_dir, img_file))\n",
    "                    labels.append(class_idx)\n",
    "    \n",
    "    print(f\"Found {len(image_paths)} images across {len(class_names)} classes\")\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(len(image_paths))\n",
    "    \n",
    "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: OAT Dataset Training (18 classes)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T10:43:09.861125Z",
     "start_time": "2025-06-29T10:43:09.858845Z"
    }
   },
   "source": [
    "# OAT dataset paths\n",
    "oat_base_path = \"../../data/OpenAnimalTracks_spokay/cropped_imgs\"\n",
    "oat_train_path = os.path.join(oat_base_path, \"train\")\n",
    "oat_val_path = os.path.join(oat_base_path, \"val\")\n",
    "oat_test_path = os.path.join(oat_base_path, \"test\")\n",
    "\n",
    "oat_class_names = sorted([d for d in os.listdir(oat_train_path)\n",
    "                         if os.path.isdir(os.path.join(oat_train_path, d))])\n",
    "print(f\"OAT classes ({len(oat_class_names)}): {oat_class_names}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OAT classes (18): ['beaver', 'black_bear', 'bob_cat', 'coyote', 'elephant', 'goose', 'gray_fox', 'horse', 'lion', 'mink', 'mouse', 'muledeer', 'otter', 'raccoon', 'rat', 'skunk', 'turkey', 'western_grey_squirrel']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T10:43:10.664152Z",
     "start_time": "2025-06-29T10:43:10.517047Z"
    }
   },
   "source": [
    "# Create OAT datasets with YOLO preprocessing\n",
    "print(\"Creating OAT training dataset with YOLO preprocessing...\")\n",
    "oat_train_ds = create_yolo_preprocessed_dataset(oat_train_path, oat_class_names, batch_size=16, shuffle=True)\n",
    "\n",
    "print(\"Creating OAT validation dataset with YOLO preprocessing...\")\n",
    "oat_val_ds = create_yolo_preprocessed_dataset(oat_val_path, oat_class_names, batch_size=16, shuffle=False)\n",
    "\n",
    "print(\"Creating OAT test dataset with YOLO preprocessing...\")\n",
    "oat_test_ds = create_yolo_preprocessed_dataset(oat_test_path, oat_class_names, batch_size=16, shuffle=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating OAT training dataset with YOLO preprocessing...\n",
      "Found 2514 images across 18 classes\n",
      "Creating OAT validation dataset with YOLO preprocessing...\n",
      "Found 346 images across 18 classes\n",
      "Creating OAT test dataset with YOLO preprocessing...\n",
      "Found 719 images across 18 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1751193790.527896   10810 gpu_device.cc:2433] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "I0000 00:00:1751193790.616192   10810 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8860 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 5070, pci bus id: 0000:01:00.0, compute capability: 12.0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T10:43:25.700572Z",
     "start_time": "2025-06-29T10:43:25.207630Z"
    }
   },
   "source": [
    "# Data augmentation for Stage 1\n",
    "data_augmentation = keras.Sequential([\n",
    "  keras.layers.Input(shape=(224, 224, 3)),\n",
    "  keras.layers.RandomFlip(\"horizontal\"),\n",
    "  keras.layers.RandomRotation(0.1),\n",
    "  keras.layers.RandomZoom(0.1),\n",
    "  keras.layers.RandomContrast(0.1),\n",
    "])\n",
    "# Create MobileNet base model\n",
    "mobilenet_base = keras.applications.MobileNetV3Small(\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\"\n",
    ")\n",
    "\n",
    "# Freeze base model for Stage 1\n",
    "mobilenet_base.trainable = False\n",
    "\n",
    "# Build Stage 1 model (18 classes)\n",
    "stage1_model = keras.Sequential([\n",
    "    data_augmentation,\n",
    "    mobilenet_base,\n",
    "    keras.layers.GlobalAveragePooling2D(),\n",
    "    keras.layers.Dense(512, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(256, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(18, activation=\"softmax\", name=\"oat_classifier\")\n",
    "], name=\"stage1_oat_model\")\n",
    "\n",
    "print(\"Stage 1 model architecture:\")\n",
    "stage1_model.summary()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 12:43:25.215873: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleLoadData(&module, data)' failed with 'CUDA_ERROR_INVALID_PTX'\n",
      "\n",
      "2025-06-29 12:43:25.215896: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleGetFunction(&function, module, kernel_name)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n",
      "\n",
      "2025-06-29 12:43:25.215906: W tensorflow/core/framework/op_kernel.cc:1829] INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n",
      "2025-06-29 12:43:25.215915: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "{{function_node __wrapped__Cast_device_/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' [Op:Cast] name: ",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mInternalError\u001B[39m                             Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Data augmentation for Stage 1\u001B[39;00m\n\u001B[32m      2\u001B[39m data_augmentation = keras.Sequential([\n\u001B[32m      3\u001B[39m   keras.layers.Input(shape=(\u001B[32m224\u001B[39m, \u001B[32m224\u001B[39m, \u001B[32m3\u001B[39m)),\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m   keras.layers.RandomFlip(\u001B[33m\"\u001B[39m\u001B[33mhorizontal\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m      5\u001B[39m   keras.layers.RandomRotation(\u001B[32m0.1\u001B[39m),\n\u001B[32m      6\u001B[39m   keras.layers.RandomZoom(\u001B[32m0.1\u001B[39m),\n\u001B[32m      7\u001B[39m   keras.layers.RandomContrast(\u001B[32m0.1\u001B[39m),\n\u001B[32m      8\u001B[39m ])\n\u001B[32m      9\u001B[39m \u001B[38;5;66;03m# Create MobileNet base model\u001B[39;00m\n\u001B[32m     10\u001B[39m mobilenet_base = keras.applications.MobileNetV3Small(\n\u001B[32m     11\u001B[39m     input_shape=(\u001B[32m224\u001B[39m, \u001B[32m224\u001B[39m, \u001B[32m3\u001B[39m),\n\u001B[32m     12\u001B[39m     include_top=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m     13\u001B[39m     weights=\u001B[33m\"\u001B[39m\u001B[33mimagenet\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     14\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/yolo_end/lib/python3.12/site-packages/keras/src/layers/preprocessing/image_preprocessing/random_flip.py:62\u001B[39m, in \u001B[36mRandomFlip.__init__\u001B[39m\u001B[34m(self, mode, seed, data_format, **kwargs)\u001B[39m\n\u001B[32m     60\u001B[39m \u001B[38;5;28msuper\u001B[39m().\u001B[34m__init__\u001B[39m(data_format=data_format, **kwargs)\n\u001B[32m     61\u001B[39m \u001B[38;5;28mself\u001B[39m.seed = seed\n\u001B[32m---> \u001B[39m\u001B[32m62\u001B[39m \u001B[38;5;28mself\u001B[39m.generator = SeedGenerator(seed)\n\u001B[32m     63\u001B[39m \u001B[38;5;28mself\u001B[39m.mode = mode\n\u001B[32m     64\u001B[39m \u001B[38;5;28mself\u001B[39m._convert_input_args = \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/yolo_end/lib/python3.12/site-packages/keras/src/random/seed_generator.py:87\u001B[39m, in \u001B[36mSeedGenerator.__init__\u001B[39m\u001B[34m(self, seed, name, **kwargs)\u001B[39m\n\u001B[32m     84\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.backend.convert_to_tensor([seed, \u001B[32m0\u001B[39m], dtype=dtype)\n\u001B[32m     86\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m.backend.name_scope(\u001B[38;5;28mself\u001B[39m.name, caller=\u001B[38;5;28mself\u001B[39m):\n\u001B[32m---> \u001B[39m\u001B[32m87\u001B[39m     \u001B[38;5;28mself\u001B[39m.state = \u001B[38;5;28mself\u001B[39m.backend.Variable(\n\u001B[32m     88\u001B[39m         seed_initializer,\n\u001B[32m     89\u001B[39m         shape=(\u001B[32m2\u001B[39m,),\n\u001B[32m     90\u001B[39m         dtype=\u001B[38;5;28mself\u001B[39m.backend.random_seed_dtype(),\n\u001B[32m     91\u001B[39m         trainable=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m     92\u001B[39m         aggregation=\u001B[33m\"\u001B[39m\u001B[33mnone\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     93\u001B[39m         name=\u001B[33m\"\u001B[39m\u001B[33mseed_generator_state\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     94\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/yolo_end/lib/python3.12/site-packages/keras/src/backend/common/variables.py:206\u001B[39m, in \u001B[36mVariable.__init__\u001B[39m\u001B[34m(***failed resolving arguments***)\u001B[39m\n\u001B[32m    204\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(initializer):\n\u001B[32m    205\u001B[39m     \u001B[38;5;28mself\u001B[39m._shape = \u001B[38;5;28mself\u001B[39m._validate_shape(shape)\n\u001B[32m--> \u001B[39m\u001B[32m206\u001B[39m     \u001B[38;5;28mself\u001B[39m._initialize_with_initializer(initializer)\n\u001B[32m    207\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    208\u001B[39m     \u001B[38;5;28mself\u001B[39m._initialize(initializer)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/yolo_end/lib/python3.12/site-packages/keras/src/backend/tensorflow/core.py:52\u001B[39m, in \u001B[36mVariable._initialize_with_initializer\u001B[39m\u001B[34m(self, initializer)\u001B[39m\n\u001B[32m     51\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_initialize_with_initializer\u001B[39m(\u001B[38;5;28mself\u001B[39m, initializer):\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m     \u001B[38;5;28mself\u001B[39m._initialize(\u001B[38;5;28;01mlambda\u001B[39;00m: initializer(\u001B[38;5;28mself\u001B[39m._shape, dtype=\u001B[38;5;28mself\u001B[39m._dtype))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/yolo_end/lib/python3.12/site-packages/keras/src/backend/tensorflow/core.py:42\u001B[39m, in \u001B[36mVariable._initialize\u001B[39m\u001B[34m(self, value)\u001B[39m\n\u001B[32m     40\u001B[39m     \u001B[38;5;28mself\u001B[39m._value = value\n\u001B[32m     41\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m42\u001B[39m     \u001B[38;5;28mself\u001B[39m._value = tf.Variable(\n\u001B[32m     43\u001B[39m         value,\n\u001B[32m     44\u001B[39m         dtype=\u001B[38;5;28mself\u001B[39m._dtype,\n\u001B[32m     45\u001B[39m         trainable=\u001B[38;5;28mself\u001B[39m.trainable,\n\u001B[32m     46\u001B[39m         name=\u001B[38;5;28mself\u001B[39m.name,\n\u001B[32m     47\u001B[39m         aggregation=\u001B[38;5;28mself\u001B[39m._map_aggregation(\u001B[38;5;28mself\u001B[39m.aggregation),\n\u001B[32m     48\u001B[39m         synchronization=\u001B[38;5;28mself\u001B[39m._map_synchronization(\u001B[38;5;28mself\u001B[39m.synchronization),\n\u001B[32m     49\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/yolo_end/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001B[39m, in \u001B[36mfilter_traceback.<locals>.error_handler\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    151\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    152\u001B[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001B[32m--> \u001B[39m\u001B[32m153\u001B[39m   \u001B[38;5;28;01mraise\u001B[39;00m e.with_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    154\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    155\u001B[39m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/yolo_end/lib/python3.12/site-packages/keras/src/backend/tensorflow/core.py:52\u001B[39m, in \u001B[36mVariable._initialize_with_initializer.<locals>.<lambda>\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     51\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_initialize_with_initializer\u001B[39m(\u001B[38;5;28mself\u001B[39m, initializer):\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m     \u001B[38;5;28mself\u001B[39m._initialize(\u001B[38;5;28;01mlambda\u001B[39;00m: initializer(\u001B[38;5;28mself\u001B[39m._shape, dtype=\u001B[38;5;28mself\u001B[39m._dtype))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/yolo_end/lib/python3.12/site-packages/keras/src/random/seed_generator.py:84\u001B[39m, in \u001B[36mSeedGenerator.__init__.<locals>.seed_initializer\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     82\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mseed_initializer\u001B[39m(*args, **kwargs):\n\u001B[32m     83\u001B[39m     dtype = kwargs.get(\u001B[33m\"\u001B[39m\u001B[33mdtype\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m---> \u001B[39m\u001B[32m84\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.backend.convert_to_tensor([seed, \u001B[32m0\u001B[39m], dtype=dtype)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/yolo_end/lib/python3.12/site-packages/keras/src/backend/tensorflow/core.py:152\u001B[39m, in \u001B[36mconvert_to_tensor\u001B[39m\u001B[34m(x, dtype, sparse, ragged)\u001B[39m\n\u001B[32m    147\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m dtype == \u001B[33m\"\u001B[39m\u001B[33mbool\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m is_int_dtype(dtype):\n\u001B[32m    148\u001B[39m         \u001B[38;5;66;03m# TensorFlow conversion is stricter than other backends, it does not\u001B[39;00m\n\u001B[32m    149\u001B[39m         \u001B[38;5;66;03m# allow ints for bools or floats for ints. We convert without dtype\u001B[39;00m\n\u001B[32m    150\u001B[39m         \u001B[38;5;66;03m# and cast instead.\u001B[39;00m\n\u001B[32m    151\u001B[39m         x = tf.convert_to_tensor(x)\n\u001B[32m--> \u001B[39m\u001B[32m152\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m tf.cast(x, dtype)\n\u001B[32m    153\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m tf.convert_to_tensor(x, dtype=dtype)\n\u001B[32m    154\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m standardize_dtype(x.dtype) == dtype:\n",
      "\u001B[31mInternalError\u001B[39m: {{function_node __wrapped__Cast_device_/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' [Op:Cast] name: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compile Stage 1 model\n",
    "stage1_model.compile(\n",
    "    optimizer=\"adamax\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Define callbacks for Stage 1\n",
    "stage1_callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train Stage 1 model\n",
    "print(\"Starting Stage 1 training on OAT dataset...\")\n",
    "stage1_history = stage1_model.fit(\n",
    "    oat_train_ds,\n",
    "    epochs=50,\n",
    "    validation_data=oat_val_ds,\n",
    "    callbacks=stage1_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save Stage 1 model\n",
    "stage1_model.save(\"mobilenet_oat_stage1.keras\")\n",
    "print(\"Stage 1 model saved as 'mobilenet_oat_stage1.keras'\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluate Stage 1 model\n",
    "print(\"Evaluating Stage 1 model...\")\n",
    "stage1_test_loss, stage1_test_accuracy = stage1_model.evaluate(oat_test_ds, verbose=1)\n",
    "print(f\"Stage 1 Test Accuracy: {stage1_test_accuracy:.4f}\")\n",
    "print(f\"Stage 1 Test Loss: {stage1_test_loss:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Real Dataset Fine-tuning (13 classes)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Real dataset paths\n",
    "real_base_path = \"../../data/dataset_no_oat_downsample_spokay\"\n",
    "\n",
    "# Get Real dataset class names (13 classes)\n",
    "real_class_names = sorted([d for d in os.listdir(real_base_path) \n",
    "                          if os.path.isdir(os.path.join(real_base_path, d))])\n",
    "print(f\"Real dataset classes ({len(real_class_names)}): {real_class_names}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create Real dataset with YOLO preprocessing\n",
    "print(\"Creating Real dataset with YOLO preprocessing...\")\n",
    "real_full_ds = create_yolo_preprocessed_dataset(real_base_path, real_class_names, batch_size=32, shuffle=True)\n",
    "\n",
    "# Split Real dataset into train/validation (80/20)\n",
    "total_batches = tf.data.experimental.cardinality(real_full_ds).numpy()\n",
    "train_size = int(0.8 * total_batches)\n",
    "val_size = total_batches - train_size\n",
    "\n",
    "real_train_ds = real_full_ds.take(train_size)\n",
    "real_val_ds = real_full_ds.skip(train_size)\n",
    "\n",
    "print(f\"Real dataset split: {train_size} train batches, {val_size} validation batches\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load Stage 1 model and extract backbone weights\n",
    "stage1_loaded = keras.models.load_model(\"mobilenet_oat_stage1.keras\")\n",
    "\n",
    "# Get the MobileNet backbone from Stage 1 model\n",
    "# Skip data augmentation layer (index 0) and get MobileNet (index 1)\n",
    "stage1_mobilenet = stage1_loaded.layers[1]\n",
    "\n",
    "# Create new MobileNet base for Stage 2 with same architecture\n",
    "stage2_mobilenet_base = keras.applications.MobileNetV3Small(\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False,\n",
    "    weights=None  # No pretrained weights, we'll transfer from Stage 1\n",
    ")\n",
    "\n",
    "# Transfer weights from Stage 1 MobileNet to Stage 2\n",
    "stage2_mobilenet_base.set_weights(stage1_mobilenet.get_weights())\n",
    "print(\"Transferred MobileNet backbone weights from Stage 1 to Stage 2\")\n",
    "\n",
    "# Unfreeze last N layers for fine-tuning\n",
    "fine_tune_at = len(stage2_mobilenet_base.layers) - 20\n",
    "stage2_mobilenet_base.trainable = True\n",
    "\n",
    "for layer in stage2_mobilenet_base.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(f\"Unfroze last {len(stage2_mobilenet_base.layers) - fine_tune_at} layers for fine-tuning\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Build Stage 2 model (13 classes) with new classification head\n",
    "stage2_model = keras.Sequential([\n",
    "    data_augmentation,\n",
    "    stage2_mobilenet_base,\n",
    "    keras.layers.GlobalAveragePooling2D(),\n",
    "    keras.layers.Dense(512, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(256, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(13, activation=\"softmax\", name=\"real_classifier\")  # 13 classes for Real dataset\n",
    "], name=\"stage2_real_model\")\n",
    "\n",
    "print(\"Stage 2 model architecture:\")\n",
    "stage2_model.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compile Stage 2 model with lower learning rate\n",
    "stage2_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Define callbacks for Stage 2\n",
    "stage2_callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train Stage 2 model\n",
    "print(\"Starting Stage 2 fine-tuning on Real dataset...\")\n",
    "stage2_history = stage2_model.fit(\n",
    "    real_train_ds,\n",
    "    epochs=60,\n",
    "    validation_data=real_val_ds,\n",
    "    callbacks=stage2_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save Stage 2 model\n",
    "stage2_model.save(\"mobilenet_real_stage2.keras\")\n",
    "print(\"Stage 2 model saved as 'mobilenet_real_stage2.keras'\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluate Stage 2 model\n",
    "print(\"Evaluating Stage 2 model...\")\n",
    "stage2_val_loss, stage2_val_accuracy = stage2_model.evaluate(real_val_ds, verbose=1)\n",
    "print(f\"Stage 2 Validation Accuracy: {stage2_val_accuracy:.4f}\")\n",
    "print(f\"Stage 2 Validation Loss: {stage2_val_loss:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot training histories\n",
    "def plot_training_history(history, title, stage_num):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    ax1.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "    ax1.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    ax1.set_title(f'{title} - Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax2.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "    ax2.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "    ax2.set_title(f'{title} - Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final metrics\n",
    "    final_train_acc = history.history['accuracy'][-1]\n",
    "    final_val_acc = history.history['val_accuracy'][-1]\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    \n",
    "    print(f\"\\n{title} Final Metrics:\")\n",
    "    print(f\"Training Accuracy: {final_train_acc:.4f}\")\n",
    "    print(f\"Validation Accuracy: {final_val_acc:.4f}\")\n",
    "    print(f\"Training Loss: {final_train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {final_val_loss:.4f}\")\n",
    "    \n",
    "# Plot both training histories\n",
    "plot_training_history(stage1_history, \"Stage 1: OAT Dataset Training\", 1)\n",
    "plot_training_history(stage2_history, \"Stage 2: Real Dataset Fine-tuning\", 2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate predictions for confusion matrix (Stage 2)\n",
    "print(\"Generating predictions for confusion matrix...\")\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for batch_images, batch_labels in real_val_ds:\n",
    "    predictions = stage2_model.predict(batch_images, verbose=0)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    y_true.extend(batch_labels.numpy())\n",
    "    y_pred.extend(predicted_classes)\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=real_class_names, \n",
    "            yticklabels=real_class_names)\n",
    "plt.title('Confusion Matrix - Stage 2 (Real Dataset)')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('True Class')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Classification report\n",
    "print(\"\\nClassification Report - Stage 2 (Real Dataset):\")\n",
    "print(\"=\" * 60)\n",
    "report = classification_report(y_true, y_pred, target_names=real_class_names, digits=4)\n",
    "print(report)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Per-class accuracy analysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "per_class_accuracy = []\n",
    "for i, class_name in enumerate(real_class_names):\n",
    "    class_mask = y_true == i\n",
    "    if np.sum(class_mask) > 0:\n",
    "        class_acc = accuracy_score(y_true[class_mask], y_pred[class_mask])\n",
    "        per_class_accuracy.append(class_acc)\n",
    "    else:\n",
    "        per_class_accuracy.append(0.0)\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "results_df = pd.DataFrame({\n",
    "    'Class': real_class_names,\n",
    "    'Accuracy': per_class_accuracy,\n",
    "    'Support': [np.sum(y_true == i) for i in range(len(real_class_names))]\n",
    "})\n",
    "\n",
    "results_df = results_df.sort_values('Accuracy', ascending=False)\n",
    "print(\"\\nPer-Class Performance:\")\n",
    "print(\"=\" * 40)\n",
    "print(results_df.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Overall metrics\n",
    "overall_accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.4f}\")\n",
    "print(f\"Mean Per-Class Accuracy: {np.mean(per_class_accuracy):.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary and Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MOBILENET TWO-STAGE TRAINING PIPELINE - FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ”§ CONFIGURATION:\")\n",
    "print(f\"- Architecture: MobileNetV3Small + Custom Classification Head\")\n",
    "print(f\"- YOLO Preprocessing: âœ“ Applied to all training data\")\n",
    "print(f\"- Stage 1 Dataset: OAT ({len(oat_class_names)} classes)\")\n",
    "print(f\"- Stage 2 Dataset: Real ({len(real_class_names)} classes)\")\n",
    "print(f\"- Backbone Weight Transfer: âœ“ From Stage 1 to Stage 2\")\n",
    "\n",
    "print(\"\\nðŸ“Š STAGE 1 RESULTS (OAT Dataset):\")\n",
    "print(f\"- Test Accuracy: {stage1_test_accuracy:.4f}\")\n",
    "print(f\"- Test Loss: {stage1_test_loss:.4f}\")\n",
    "print(f\"- Epochs Trained: {len(stage1_history.history['loss'])}\")\n",
    "\n",
    "print(\"\\nðŸ“Š STAGE 2 RESULTS (Real Dataset):\")\n",
    "print(f\"- Validation Accuracy: {stage2_val_accuracy:.4f}\")\n",
    "print(f\"- Validation Loss: {stage2_val_loss:.4f}\")\n",
    "print(f\"- Epochs Trained: {len(stage2_history.history['loss'])}\")\n",
    "print(f\"- Overall Test Accuracy: {overall_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ’¾ SAVED MODELS:\")\n",
    "print(\"- mobilenet_oat_stage1.keras (Stage 1: OAT training)\")\n",
    "print(\"- mobilenet_real_stage2.keras (Stage 2: Real fine-tuning)\")\n",
    "\n",
    "print(\"\\nâœ… PIPELINE COMPLIANCE:\")\n",
    "print(\"- Follows MobileNet_training.md specifications: âœ“\")\n",
    "print(\"- Mandatory YOLO preprocessing: âœ“\")\n",
    "print(\"- Two-stage training (OAT â†’ Real): âœ“\")\n",
    "print(\"- Architecture adaptation (18 â†’ 13 classes): âœ“\")\n",
    "print(\"- Backbone weight transfer: âœ“\")\n",
    "print(\"- Comprehensive evaluation: âœ“\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

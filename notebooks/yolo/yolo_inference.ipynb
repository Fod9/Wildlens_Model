{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-25T19:45:46.668537Z",
     "start_time": "2025-06-25T19:45:46.666722Z"
    }
   },
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.engine.results import Results\n",
    "from ultralytics.engine.results import Boxes\n",
    "from typing import List\n",
    "from PIL import Image\n"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T19:45:47.084214Z",
     "start_time": "2025-06-25T19:45:47.067567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the model from weights\n",
    "\n",
    "model = YOLO('best_so_far.pt')"
   ],
   "id": "a80ff9a8bd7ac0e8",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T19:45:47.570943Z",
     "start_time": "2025-06-25T19:45:47.518338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inference on a single image\n",
    "\n",
    "test_images = [\n",
    "    'test_images/footprint_1.jpeg',\n",
    "]\n",
    "\n",
    "results = model.predict(\n",
    "    source=test_images[0],\n",
    "    conf=0.25,  # Confidence threshold\n",
    "    iou=0.45,   # IoU threshold\n",
    "    save_conf=True,  # Save confidence scores\n",
    ")\n",
    "\n",
    "print(results)"
   ],
   "id": "6ec8819bcb016fe6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/spokay/projects/pycharms/Wildlens_Model/notebooks/yolo/test_images/footprint_1.jpeg: 352x640 2 items, 4.2ms\n",
      "Speed: 0.9ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Results saved to \u001B[1mruns/detect/predict2\u001B[0m\n",
      "1 label saved to runs/detect/predict2/labels\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'item'}\n",
      "obb: None\n",
      "orig_img: array([[[ 85,  81,  92],\n",
      "        [ 26,  25,  35],\n",
      "        [ 33,  32,  42],\n",
      "        ...,\n",
      "        [112, 109, 111],\n",
      "        [144, 140, 145],\n",
      "        [153, 149, 154]],\n",
      "\n",
      "       [[134, 130, 141],\n",
      "        [ 85,  84,  94],\n",
      "        [ 60,  61,  71],\n",
      "        ...,\n",
      "        [111, 106, 108],\n",
      "        [165, 160, 162],\n",
      "        [182, 177, 179]],\n",
      "\n",
      "       [[129, 128, 138],\n",
      "        [101, 102, 112],\n",
      "        [ 91,  92, 102],\n",
      "        ...,\n",
      "        [168, 162, 163],\n",
      "        [164, 158, 159],\n",
      "        [122, 116, 117]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 72,  94, 112],\n",
      "        [ 88, 110, 128],\n",
      "        [ 59,  81,  99],\n",
      "        ...,\n",
      "        [142, 154, 166],\n",
      "        [131, 143, 155],\n",
      "        [202, 214, 226]],\n",
      "\n",
      "       [[ 29,  53,  71],\n",
      "        [ 67,  91, 109],\n",
      "        [100, 124, 142],\n",
      "        ...,\n",
      "        [134, 143, 156],\n",
      "        [153, 162, 175],\n",
      "        [220, 229, 242]],\n",
      "\n",
      "       [[124, 148, 166],\n",
      "        [ 92, 116, 134],\n",
      "        [101, 125, 143],\n",
      "        ...,\n",
      "        [153, 162, 175],\n",
      "        [130, 138, 151],\n",
      "        [150, 158, 171]]], shape=(386, 736, 3), dtype=uint8)\n",
      "orig_shape: (386, 736)\n",
      "path: '/home/spokay/projects/pycharms/Wildlens_Model/notebooks/yolo/test_images/footprint_1.jpeg'\n",
      "probs: None\n",
      "save_dir: 'runs/detect/predict2'\n",
      "speed: {'preprocess': 0.8781559999988531, 'inference': 4.162530000030529, 'postprocess': 0.6897270004628808}]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T19:45:48.124283Z",
     "start_time": "2025-06-25T19:45:48.117020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check bounding boxes\n",
    "print(results[0].boxes)\n",
    "print(results[0].boxes.shape)"
   ],
   "id": "3bc55dac5abf6bd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0.], device='cuda:0')\n",
      "conf: tensor([0.7834, 0.5766], device='cuda:0')\n",
      "data: tensor([[3.5629e+02, 3.1329e+01, 6.0497e+02, 2.5727e+02, 7.8338e-01, 0.0000e+00],\n",
      "        [9.2517e+01, 8.2934e+01, 2.9249e+02, 2.7693e+02, 5.7661e-01, 0.0000e+00]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (386, 736)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[480.6293, 144.3003, 248.6881, 225.9423],\n",
      "        [192.5028, 179.9319, 199.9716, 193.9949]], device='cuda:0')\n",
      "xywhn: tensor([[0.6530, 0.3738, 0.3379, 0.5853],\n",
      "        [0.2616, 0.4661, 0.2717, 0.5026]], device='cuda:0')\n",
      "xyxy: tensor([[356.2852,  31.3291, 604.9734, 257.2715],\n",
      "        [ 92.5170,  82.9344, 292.4886, 276.9293]], device='cuda:0')\n",
      "xyxyn: tensor([[0.4841, 0.0812, 0.8220, 0.6665],\n",
      "        [0.1257, 0.2149, 0.3974, 0.7174]], device='cuda:0')\n",
      "torch.Size([2, 6])\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T19:45:48.732265Z",
     "start_time": "2025-06-25T19:45:48.728181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check confidence scores\n",
    "print(results[0].boxes.conf)"
   ],
   "id": "a0431d00c8f0cd1b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7834, 0.5766], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calculate areas of bounding boxes\n",
    "normalize_bbox = lambda bbox: [float(coord) for coord in bbox]\n",
    "\n",
    "def calculer_areas_bbox_normalised(x1: float, y1: float, x2: float, y2: float):\n",
    "    largeur = abs(x2 - x1)\n",
    "    hauteur = abs(y2 - y1)\n",
    "    return largeur * hauteur\n",
    "\n",
    "def get_areas_from_bboxes(bbox: Boxes) -> List[float]:\n",
    "    aires = []\n",
    "    for bbox in bbox.xyxy:\n",
    "        # Calculate area\n",
    "        x1, y1, x2, y2 = normalize_bbox(bbox)\n",
    "        aire = calculer_areas_bbox_normalised(x1, y1, x2, y2)\n",
    "        aires.append(aire)\n",
    "\n",
    "    return aires"
   ],
   "id": "5686a69bb7f4159e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get an average of the areas and the confidence scores\n",
    "@dataclass\n",
    "class BBoxWithScore:\n",
    "    bbox: List[float]\n",
    "    score: float\n",
    "\n",
    "def get_scrores_from_areas_and_confidence(areas: List[float], confidences: List[float]) -> List[float]:\n",
    "    if len(areas) != len(confidences):\n",
    "        raise ValueError(\"Areas and confidences must have the same length.\")\n",
    "    length = len(areas)\n",
    "    scores = []\n",
    "    for area, confidence in zip(areas, confidences):\n",
    "        score = area * confidence / length if length > 0 else 0.0\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_average_area_and_confidence(bboxes: Boxes) -> List[BBoxWithScore] | None:\n",
    "    if bboxes is None or len(bboxes.xyxy) == 0:\n",
    "        return None\n",
    "    areas = get_areas_from_bboxes(bboxes)\n",
    "\n",
    "    confidences = bboxes.conf.tolist()\n",
    "    scores = get_scrores_from_areas_and_confidence(areas, confidences)\n",
    "    bboxes_with_scores = [\n",
    "        BBoxWithScore(bbox=bbox.tolist(), score=score)\n",
    "        for bbox, score in zip(bboxes.xyxy, scores)\n",
    "    ]\n",
    "\n",
    "    return bboxes_with_scores"
   ],
   "id": "62e3376e2b6036bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get the best bounding box with the highest score\n",
    "def get_best_bbox(bboxes_with_scores: List[BBoxWithScore]) -> BBoxWithScore | None:\n",
    "    if not bboxes_with_scores:\n",
    "        return None\n",
    "    best_bbox = max(bboxes_with_scores, key=lambda x: x.score)\n",
    "    return best_bbox"
   ],
   "id": "b36563a879550eb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check the BBOX results by cropping the images\n",
    "\n",
    "def get_bboxes_from_results(result: Results) -> List[Boxes]:\n",
    "    return result.boxes\n",
    "\n",
    "def crop_images_from_results(results: List[Results], image_paths: List[str]) -> List[Image]:\n",
    "    cropped_images = []\n",
    "    for result, image_path in zip(results, image_paths):\n",
    "        img = Image.open(image_path)\n",
    "        bboxes = get_bboxes_from_results(result)\n",
    "        for bbox in bboxes.xyxy:  # xyxy format\n",
    "            x1, y1, x2, y2 = map(int, bbox[:4])  # Convert to int\n",
    "            cropped_image = img.crop((x1, y1, x2, y2))  # Crop the image\n",
    "            cropped_images.append(cropped_image)  # Append cropped image to list\n",
    "\n",
    "    return cropped_images\n",
    "\n",
    "\n",
    "test_cropped_images = crop_images_from_results(results, test_images)\n",
    "\n",
    "# Display cropped images\n",
    "for i, cropped_image in enumerate(test_cropped_images):\n",
    "    cropped_image.show(title=f\"Cropped Image {i+1}\")\n",
    "    # Optionally save the cropped images\n",
    "    cropped_image.save(f\"cropped_image_{i+1}.jpg\")"
   ],
   "id": "e29cb082626c675b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
